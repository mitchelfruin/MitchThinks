---
title: "Slow thinking about small samples."
author: "Mitchel Fruin"
date: "2019-12-31"
description: "Data science meets behavioural economics to explain why small samples generate extreme results more often."
tags: ["Behavioural Economics", "Probability and Statistics"]
slug: slow_thinking_small_samples
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    number_sections: true
---

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r setup, include=FALSE}
# RMarkdown
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8.09,
                      fig.height = 5)

# Packages
library(tidyverse)
library(magrittr)
library(funnelR)
```

In 2006, the statistician Howard Wainer and the education researcher Harris Zwerling wrote a short [research paper](https://www.pdkmembers.org/members_online/publications/Archive/pdf/k0612wai.pdf) in which they present two facts about the rate of kidney cancer in US counties. Do me the favour of pausing after reading each and thinking about how you would explain them. 

First, 

> Counties that are in the lowest 10% for kidney cancer incidence tend to be rural and located in the Midwest, the South, and the West.

If you're like most people, you might have been tempted to explain the low cancer rate with "the clean living of the rural lifestyle - no air pollution, no water pollution, and access to fresh food without additives".

Second,

> Counties that are in the highest 10% for kidney cancer incidence tend to be rural and located in the Midwest, the South, and the West.

Again, you might have been tempted to appeal to the rural lifestyle for an explanation. But, this time, to suggest that the high cancer rates might be due to "limited access to good medical care, a high-fat diet, too much alcohol, and too much tobacco."

Clearly, something is off with our gut instinct here. The rural lifestyle can't simultaneously explain both high and low incidence of kidney cancer.

This pair of facts illustrate the concept of **sample size neglect**, a failure to accurately account for sample size when making inferences from data. The outcome is that people tend to believe a small sample will be more representative of the true phenomenon than statistical theory predicts it will be.  

So, the key here isn't the *behaviour* of people who live in rural areas, it's the *number* of people who live in rural areas.   

# Small samples are more extreme

Sampling is a foundational concept within statistics. The vast majority of the time the population we're interested in is so large that it's impractical and/or impossible to study it as a whole. Instead, to reduce the burden of the analysis, we take a proportion of the population and use that sample to make inferences. One key to using the sampling process properly is remembering the mantra that small samples generate extreme results more often. 

To see this, consider the following example. Imagine we have two maternity wards. One, at Hospital Large, has 8 births each day. The other, at Hospital Small, has 4. We're interested in comparing extreme results. In this context, that equates to a 'single-sex day' where all the babies born are the same sex. Intuitively, we understand *that* single-sex days will happen more often at Hospital Small for the same reason that we know instinctively that it's easier to flip 4 heads in a row than it is 8. However, what we don't seem to grasp so intuitively (which underlies sample size neglect), is *how much* more often single-sex days will happen. 

Actually consider your answer: how much more often will Hospital Small's maternity ward have a single-sex day than Hospital Large's?

We can find an exact answer to this question by comparing two binomial distributions. The random variables $S$ and $L$ denote the number of boys born each day at Hospital Small and Large respectively.

$$\begin{aligned}
S&\sim \text{B}(4, 0.5) \\
L&\sim \text{B}(8, 0.5)
\end{aligned}$$

We're interested in the probability of the extreme outcomes (single-sex days) at each hospital. That is, we want to find out how likely it is that the children born are either all boys or all girls.

For Hospital Large:

$$\begin{aligned}
\text{Pr}(L = 0 \;\text{or}\; L = 8) &= \text{Pr}(L = 0) + \text{Pr}(L = 8) \\
&= {8 \choose 0}\times 0.5^0 \times 0.5^8 + {8 \choose 8}\times 0.5^8 \times 0.5^0 \\
&= 0.00390625 + 0.00390625 \\
&= 0.0078125
\end{aligned}$$

This means we can expect 0.78125% of days at Hospital Large to be single-sex. 

For Hospital Small:

$$\begin{aligned}
\text{Pr}(S = 0 \;\text{or}\; S = 4) &= \text{Pr}(S = 0) + \text{Pr}(S = 4) \\
&= {4 \choose 0}\times 0.5^0 \times 0.5^4 + {4 \choose 4}\times 0.5^4 \times 0.5^0 \\
&= 0.0625 + 0.0625 \\
&= 0.125
\end{aligned}$$

This means we can expect 12.5% of days at Hospital Small to be single-sex. 

To find out how much more often Hospital Small's maternity ward will have a single-sex day than Hospital Large's we just need to do some division:

$$\begin{aligned}
\frac{12.5}{0.78125} = 16
\end{aligned}$$

```{r, eval = F}
large_exact <- dbinom(8, 8, 0.5)*2
small_exact <- dbinom(4, 4, 0.5)*2
small_exact/large_exact
```

So, Hospital Small, with its 4 births per day, will have 16 times more single-sex days than Hospital Large. How does that compare with your guess?

If that was too much maths, then we can run a simulation to get an approximate answer instead. Let's simulate 1000 days at each hospital and plot the frequencies of the number of boys born per day. 

```{r}
# Simulate 1000 days worth of births at each hospital
set.seed(5)
small_hosp <- rbinom(1000, 4, 0.5)
large_hosp <- rbinom(1000, 8, 0.5)
sims <- tibble(hospital = c(rep("Hospital Small", 1000),
                            rep("Hospital Large", 1000)),
               births = c(small_hosp, large_hosp)) %>%
  count(hospital, births) %>%
  mutate(extreme = if_else(births %in% c(0, 8),
                           T,
                           if_else(hospital == "Hospital Small" & 
                                   births == 4,
                                   T, 
                                   F),
                           F))

# Plot
ggplot(sims, aes(x = births, y = n, fill = extreme)) +
  geom_col() +
  scale_x_continuous("Number of boys born", breaks = seq(0, 8, 1)) +
  scale_y_continuous("Number\nof days") +
  scale_fill_viridis_d("Single-sex\nday") +
  facet_grid(~ hospital, scales = "free_x", space = "free_x") +
  theme_light() +
  theme(title = element_text(size = 14),
        text = element_text(size = 12),
        strip.text = element_text(size = 14),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```

```{r}
# Find the single sex days
small_extreme <- sum(small_hosp == 4| small_hosp == 0)
large_extreme <- sum(large_hosp == 8| large_hosp == 0)
sim_ratio <- small_extreme/large_extreme
```

Adding up the yellow bars shows us that there were `r small_extreme` single-sex days at Hospital Small and only `r large_extreme` at Hospital Large. This means that single-sex days occurred roughly `r round(sim_ratio)` times more often at Hospital Small, which isn't too far away from the exact answer of 16.

So, we've established that a smaller sample size (4 rather than 8 in our example) generates extreme results (single-sex days) more often than we might naturally expect. This means our fast thinking can lead us astray a little and many of us tend to underestimate just how variable small samples will be. What can we do about it? How can we remind ourselves that we should expect small samples to be more extreme when analysing data?

# Visualise the expected variation

I recently came across an example from the UK in a similar vein to the one presented by Wainer and Zwerling. In *The Art of Statistics* David Spiegelhalter discusses a piece of analysis by Paul Barden. Barden saw a BBC news report headlined "3-Fold Variation in Bowel Cancer Rate" and decided to dig a little deeper (discussed on his own blog [here](https://pb204.blogspot.com/2011/09/im-grateful-to-david-spiegelhalter-of.html)).

```{r}
# Import data
bowel_data <- read.csv("https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/09-2-funnel-bowel-cancer/09-2-bowel-cancer-data-x.csv")

# Add death rate
bowel_data %<>% mutate(prop_death = n/d)
mean_prop <- mean(bowel_data$prop_death)
max_prop <- max(bowel_data$prop_death)
min_prop <- min(bowel_data$prop_death)

# Numerator must be called n, denomnator d
funnel_limits <- fundata(input = bowel_data,
                         alpha = 0.95,
                         alpha2 = 0.998,
                         method = 'approximate',
                         step = 1)

# Identify Glasgow City in data frame
glasgow <- subset(bowel_data, District == "Glasgow City")
```

It turns out the headline is true. In fact, there's more than 3-fold variation. The highest mortality rate of `r round(max(bowel_data$prop_death)*100000)` in 100,000 in Glasgow is `r round(max_prop/min_prop, 2)` times that of the minimum rate found in The Borough of Rossendale.

But, as we established above, small samples generate extreme results more often. So, we need to incorporate the idea that the cancer rate will be more variable in areas with smaller population sizes in our analysis. The easiest way to do this is with a **funnel plot** which visualises the expected variation at each sample size given the overall population mean.

```{r}
# Find funnel limits for lowest pop of 31332
up_limit <- funnel_limits %>%
  filter(d == 31331) %>%
  pull(up2)
low_limit <- funnel_limits %>%
  filter(d == 31331) %>%
  pull(lo2)

# Funnel plot
ggplot() +
  geom_ribbon(aes(x = d, ymin = lo, ymax = up),
              data = funnel_limits,
              fill = "#440154FF",
              alpha = 0.3) +
  geom_ribbon(aes(x = d, ymin = lo2, ymax = up2),
              data = funnel_limits,
              fill = "#440154FF",
              alpha = 0.15) +
  geom_point(aes(x = d, y = prop_death),
             data = bowel_data) +
  geom_smooth(aes(x = d, y = prop_death),
              data = bowel_data,
              method = "lm",
              se = F,
              col = "#FDE725FF", 
              lty = 2) +
  geom_text(aes(x = d, y = prop_death, label = "Glasgow"),
            data = glasgow,
            hjust = -0.05,
            vjust = 1) +
  scale_x_continuous("Population (100,000s)",
                     breaks = 100000*(0:14),
                     labels = 0:14,
                     limits = c(0, max(bowel_data$d))) +
  scale_y_continuous("Annual bowel cancer mortality rate per 100,000",
                     breaks = 5*(0:8)/100000,
                     labels = 5*(0:8),
                     limits = c(low_limit, up_limit)) +
  theme_classic() +
  theme(title = element_text(size = 14),
        text = element_text(size = 12),
        panel.grid.major = element_line())
```

The dashed yellow line represents a linear model of cancer rate as a function of population size. The fact that it's virtually horizontal suggests there's no simple, global relationship. The dark and light purple bands represent 95% and 99.8% confidence intervals respectively. They visualise the variation we expect at each population size simply as a statistical inevitability. Glasgow sticks out as a prominent outlier and there are probably a few too many points outside the 95% interval but, on the whole, the data points fall pretty neatly within the bounds. There may well be regional differences in the bowel cancer rate, but the funnel plot reveals that in terms of baseline variation there isn't anything more interesting going on than in our simulation of the two imaginary hospitals. Perhaps the headline should've instead read, "Variation in Bowel Cancer Rate About Right". 

# Stats as slow thinking

The metaphor of two modes of thinking, one fast and intuitive but prone to some predictable mistakes and the other slow and deliberate but sometimes hard to engage, has become fairly mainstream since Kahneman’s *Thinking, Fast and Slow* and the popularisation of behavioural economics. This metaphor is often used when discussing the ‘mistakes’ people are susceptible to. However, accepting the fallibility of our intuitions is just the start of good decision-making, not an end in itself. 

The next step is to consider just what kind of slow thinking we should employ in a given situation. The simplest option might be a pros and cons list, or we could use an improper linear model if we want to get a little more fancy. Often, however, these won't suffice and the slow thinking required is formal statistical analysis.

This is where my interest in behavioural economics meets my interest in data science. To me, a useful way of framing the role of data science is as painfully slow thinking. It requires taking the right care to clean and format the data, choosing an appropriate method, checking the assumptions are valid, testing the sensitivity of your results, and ensuring you aren't deluding yourself with your interpretation. 

When making large scale policy decisions, engaging in such painfully slow thinking is crucial. If you don't, then you run the risk of misattributing run-of-the-mill statistical variation to an incorrect, but compelling, narrative like the wonders (and/or horrors) of the rural lifestyle.

For example, the Wainer and Zwerling article I began this post with focused on a $1.7 billion investment by the Bill & Melinda Gates foundation to implement research into what makes schools successful. One finding was that amongst the most successful schools, small schools are over-represented. This led the foundation, along with the U.S Department of Education Smaller Learning Communities Program, to make substantial investment in the creation of smaller schools. It's easy to construct a story to explain why small schools cause better educational outcomes, perhaps by suggesting there might be more personal attention or individual encouragement for each student. However, the crucial fact, that painfully slow thinking like the production of a funnel plot would hopefully help you realise, is that small schools are also over-represented when you look at the worst schools. Why? Because (please remember this if you ever get an important job that affects millions of people's lives) small samples produce extreme results more often. 
