---
title: "Slow thinking about small samples."
author: "Mitchel Fruin"
date: "2019-10-26"
description: "Add description."
tags: ["Behavioural Economics", "Probability and Statistics"]
slug: slow_thinking_small_samples
draft: true
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    number_sections: true
---

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r setup, include=FALSE}
# RMarkdown
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8.09,
                      fig.height = 5)

# Packages
library(tidyverse)
library(magrittr)
library(funnelR)
```

In 2006, the statistician Howard Wainer and the education researcher Harris Zwerling wrote a short [research paper](https://www.pdkmembers.org/members_online/publications/Archive/pdf/k0612wai.pdf) in which they present two facts about the rate of kidney cancer in US counties. Do me the favour of pausing after reading each and thinking about how you would explain them. 

First, 

> Counties that are in the lowest 10% for kidney cancer incidence tend to be rural and located in the Midwest, the South, and the West.

If you're like most people, you might have been tempted to explain the low cancer rate with "the clean living of the rural lifestyle - no air pollution, no water pollution, and access to fresh food without additives".

Second,

> Counties that are in the highest 10% for kidney cancer incidence tend to be rural and located in the Midwest, the South, and the West.

Again, you might have been tempted to appeal to the rural lifestyle for an explanation. But this time, to suggest that the high cancer rates might be due to "limited access to good medical care, a high-fat diet, too much alcohol, and too much tobacco."

Clearly, something is off with our gut instinct here. The rural lifestyle can't simultaneously explain both high and low incidence of kidney cancer.

This pair of facts illustrate the concept of **sample size neglect**, a failure to acccurately account for sample size when making inferences from data. This failing stems from the representativeness heuristic. It manifests when people don't know the process by which the data is being generated (in this case what causes high or low rates of kidney cancer). The outcome is that people tend to believe a small sample will be more representative of the true phenomenon than statistical theory predicts it actually will be.  

So, the key here isn't the *behaviour* of people who live in rural areas, it's the *number* of people who live in rural areas.   

# Small samples are more extreme

Sampling is a foundational concept within statistics. The vast majority of the time the population we're interested in is so large that it's impractical and/or impossible to study it as a whole. Instead, to reduce the burden of the analysis, we take a proportion of the population and use that sample to make inferences. One key to using the sampling process properly is remembering the mantra that small samples generate extreme results more often. 

To see demonstrate this, consider the following example. Imagine we have two maternity wards. One, at Hospital Large, has 8 births each day. The other, at Hospital Small, has 4. We're interested in comparing extreme results. In this context, that equates to a 'single-sex day' where all the babies born are the same sex. Intuitively, we understand *that* single-sex days will happen more often at Hospital Small for the same reason that we know instinctively that it's easier to flip 4 heads in a row than it is 8. However, what we don't seem to grasp so intuitively (which underlies sample size neglect), is *how much* more often single-sex days will happen. 

Actually consider your answer, how much more often will Hospital Small's maternity ward have a single-sex day than the Hospital Large's?

We can find an exact answer to this question by comparing two binomial distributions, $S$ and $L$.

$$\begin{aligned}
S&\sim \text{B}(4, 0.5) \\
L&\sim \text{B}(8, 0.5)
\end{aligned}$$

Add in binomial formula here.

```{r}
large_exact <- dbinom(8, 8, 0.5)*2
small_exact <- dbinom(4, 4, 0.5)*2
small_exact/large_exact
```

We see that Hospital Small, with its 4 births per day, will have 16 times more single-sex days than Hospital Large. How does that compare with your guess?

If that was too much maths, then we can run a simulation to get an approximate answer instead. Let's simulate 1000 days at each hospital and plot the frequencies of the number of boys born per day. 

```{r}
# Simulate 1000 days worth of births at each hospital
set.seed(5)
small_hosp <- rbinom(1000, 4, 0.5)
large_hosp <- rbinom(1000, 8, 0.5)
sims <- tibble(hospital = c(rep("Hospital Small", 1000),
                            rep("Hospital Large", 1000)),
               births = c(small_hosp, large_hosp)) %>%
  count(hospital, births) %>%
  mutate(extreme = if_else(births %in% c(0, 8),
                           T,
                           if_else(hospital == "Hospital Small" & 
                                   births == 4,
                                   T, 
                                   F),
                           F))

# Plot
ggplot(sims, aes(x = births, y = n, fill = extreme)) +
  geom_col() +
  scale_x_continuous("Number of boys born", breaks = seq(0, 8, 1)) +
  scale_y_continuous("Number\nof days") +
  scale_fill_viridis_d("Single-sex\nday") +
  facet_grid(~ hospital, scales = "free_x", space = "free_x") +
  theme_light() +
  theme(title = element_text(size = 14),
        text = element_text(size = 12),
        strip.text = element_text(size = 14),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```

```{r}
# Find the single sex days
small_extreme <- sum(small_hosp == 4| small_hosp == 0)
large_extreme <- sum(large_hosp == 8| large_hosp == 0)
sim_ratio <- small_extreme/large_extreme
```

Adding up the yellow bars shows us that there were `r small_extreme` single-sex days at Hospital Small and only `r large_extreme` at Hospital Large. This means that single-sex days occurred roughly `r round(sim_ratio)` times as often at Hospital Small, which isn't too far away from the exact answer of 16.

So, we've established that a smaller sample size (4 rather than 8 in our example) generates extreme results (single-sex days) more often than we naturally expect. This means our fast thinking can lead us astray a little and many of us tend to underestimate just how variable small samples will be. What can we do about it? How can we remind ourselves that we should expect small samples to be more extreme when analysing data?

# Visualise the expected variance

I recently came across an example from the UK in a similar vein to the one discussed by Wainer and Zwerling. In *The Art of Statistics* David Spiegelhalter discusses a piece of analysis by Paul Barden. Barden saw a BBC news report headlined "3-Fold Variation in Bowel Cancer Rate" and decided to dig a little deeper (discussed on his own blog [here](https://pb204.blogspot.com/2011/09/im-grateful-to-david-spiegelhalter-of.html)).

```{r}
# Import data
bowel_data <- read.csv("https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/09-2-funnel-bowel-cancer/09-2-bowel-cancer-data-x.csv")

# Add death rate
bowel_data %<>% mutate(prop_death = n/d)
mean_prop <- mean(bowel_data$prop_death)
max_prop <- max(bowel_data$prop_death)
min_prop <- min(bowel_data$prop_death)

# Numerator must be called n, denomnator d
funnel_limits <- fundata(input = bowel_data,
                         #benchmark = mean_prop,
                         alpha = 0.95,
                         alpha2 = 0.998,
                         method = 'exact',
                         step = 10)

# Identify Glasgow City in data frame
glasgow <- subset(bowel_data, District == "Glasgow City")
```

It turns out the headline is true. In fact, there's more than 3-fold variation. The highest mortality rate of `r round(max(bowel_data$prop_death)*100000)` in 100,000 in Glasgow is `r round(max_prop/min_prop, 2)` times that of the minimum rate found in The Borough of Rossendale.

But, as we established above, small samples generate extreme results more often. So, we need to incorporate population size in our analysis. The easiest way to do this is with a ‘funnel plot’ which visualises the mean and expected variation from this mean given sample size.

```{r}
# Find funnel limits for lowest pop of 31332
up_limit <- filter(funnel_limits, d == 31331) %>%
  pull(up2)
low_limit <- filter(funnel_limits, d == 31331) %>%
  pull(lo2)

# Funnel plot
ggplot() +
  geom_point(aes(x = d, y = prop_death), data = bowel_data) +
  geom_hline(yintercept = mean_prop) +
  geom_ribbon(aes(x = d, ymin = lo, ymax = up),
              data = funnel_limits,
              fill = "blue",
              alpha = 0.4) +
  geom_ribbon(aes(x = d, ymin = lo2, ymax = up2),
              data = funnel_limits,
              fill = "purple",
              alpha = 0.2) +
  scale_y_continuous("Annual bowel cancer mortality rate per 100,000",
                     breaks = 5*(0:8)/100000,
                     labels = 5*(0:8),
                     limits = c(low_limit, up_limit)) + 
  scale_x_continuous("Population (100,000's)",
                     breaks = 100000*(0:14),
                     labels = 0:14,
                     limits = c(0, max(bowel_data$d))) +
  theme_classic() +
  theme(panel.grid.major = element_line()) + 
  annotate("text", 
           x = glasgow$d,
           y = glasgow$n/glasgow$d,
           label = "Glasgow City",
           hjust = -0.05,
           vjust = 1)
```

Sentence interpreting graph. 

We have a suspiciously good match to the expected variability with the vast majority of our datapoints falling within the bounds of the confidence intervals (Glasgow is the only apparent outlier). This suggests that there likely isn't anything more interesting going on in general than in our simulation of the two hospitals above. Perhaps the headline should've instead read, "Variation in Bowel Cancer Rate About Right". 

# Stats as slow thinking

The metaphor of two modes of thinking, one fast and intuitive but prone to some predictable mistakes and the other slow and deliberate but sometimes hard to engage, has become fairly mainstream since Kahneman’s *Thinking, Fast and Slow* and the popularisation of behavioural economics. This metaphor is often used when discussing the ‘mistakes’ people are susceptible to. However, accepting the fallibility of our own reasoning is just the start of good decision-making, not an end in itself. 

The next step is to consider just what kind of slow thinking we should employ in a given situation.The simplest option might be a pros and cons list, or we could use an improper linear model if we want to get a little more sophisticated. However, often the slow thinking required is formal statistical analysis.

This is where my interest in behavioural economics meets my interest in data science. To me, data science can be cast as painfully slow thought involving taking the right care to clean and format the data, choosing an appropriate method, checking statistical assumptions, testing the sensitivity of your results, and ensuring you aren't deluding yourself with your interpretation. 

Particularly when making large scale policy decisions, engaging in such painfully slow thinking is crucial. If you don't, then you run the risk of misattributing run-of-the-mill statistical variation to an incorrect, but compelling, narrative like the wonders (or horrors) of the rural lifestyle. For example, Bill & Melinda Gates Foundation example of it gone wrong.
