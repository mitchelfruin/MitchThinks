---
title: "Chaos with a pocket calculator."
author: "Mitchel Fruin"
date: "2019-09-25"
description: "Introduction to chaos theory using the simple population growth example from Robert May's classic paper."
tags: ["Complexity", "Interactive Visualisation"]
slug: chaos_logistic_map
draft: true
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    number_sections: true
---

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r setup, include=FALSE}
# RMarkdown
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8.09,
                      fig.height = 5)

# Packages
library(tidyverse)
library(plotly)
library(shiny)
```

Most non-fiction books teach you **what** to think. They give you a new set of facts to trot out when someone brings up pop psychology at the pub or let you feel smug when you 'explain' why the Bank of England cut interest rates. Every now and then, however, you read a book that changes **how** you think, one that helps you see things beyond its few hundred pages slightly differently. That's what James Gleick's *Chaos* did for me. It's fantastic, I wish I'd read it much, much sooner, and anyone even remotely interested in science should make it their next book.

Instead of butchering a full explanation of chaos, I thought I'd run through an example on my home turf. Chapter 3 discusses chaos in the context of population biology by, in large part, recounting a [1976 paper](http://abel.harvard.edu/archive/118r_spring_05/docs/may.pdf) from Robert May. I heard a guest lecture from him once, describing how studying complex ecosystems had helped him advise on the re-design of the banking system. It all went a fair distance over my head at the time. But, I think a combination of recognising his name, familiarity with population biology, and the memory of faculty members' deference for May, made the chapter stick in my mind. Stickiest of all was Gleick's summary of May's argument:

> The world would be a better place... if every young student were given a pocket calculator and encouraged to play with the logistic difference equation. That simple calculation... could counter the distorted sense of the world's possibilities that comes from a standard scientific education.

Well, R is just a fancy pocket calculator. So, we best start playing.

# The logistic map

Biologists often explore how a group of individuals of a single species changes over time (a 'population's dynamics'). The simplest way to do this is to consider time as being discrete. This means thinking in terms of the changes from one chunk of time (day, week, season, year) to the next, rather than in continuous underlying time. 

This simplification actually describes species with non-overlapping generations accurately. For example, EXAMPLE. In this case, 'studying dynamics' often means finding the mathemetical rules that tell us what we should expect the total number of individuals in the next generation, $N_{t+1}$, to be given how many individuals there are in this generation, $N_t$. This post does include some equations but just think of them all in this light, they're just rules telling you how many EXAMPLE you'll have next year if this year you've got X.

So, say we're looking for an equation describing a population's dynamics from one year to the next. The simplest option might be to say that next year's population depends only on this year's and nothing else. This would give us the following equation:

$$N_{t+1} = rN_t$$

which says that the number of individuals next year, $N_{t+1}$, is just the number of individuals this year, $N_t$, multiplied by some parameter that encapsulates the growth rate, $r$. The growth rate $r$ could be influenced by the climate, the species life-history evolution, or anything else you can think of. Just for clarity, if you have 100 individuals this year and $r=0.5$, then next year you'll have 50.   

Let's explore three populations starting with 100 individuals which have different growth rates. 

```{r}
# Single input function
single_growth <- function(start_x, a, n) {
  
  # Empty dataframe
  pop_df <- tibble(generation = 1:n,
                   x = numeric(n))
  
  # Add first generation proportion
  pop_df$x[1] <- start_x
  
  # Simulate n generations
  for (i in 2:n) {
    pop_df$x[i] <- a*pop_df$x[i-1]
  }
  
  # Final dataframe
  pop_df
}

# Multiple input function
multiple_growth <- function(x_opts, a_opts, n_opts) {
  
  # Inputs should all be vectors
  
  # Find all possible input permutations
  inputs_df <- expand.grid("start_x" = x_opts,
                           "a" = a_opts,
                           "n" = n_opts)
  
  # Blank outcome df 
  outcome_df <- tibble(start_x = numeric(),
                       a = numeric(),
                       n = numeric(),
                       generation = numeric(), 
                       x = numeric())
  
  # Use single input function for all permutations
  for (i in 1:nrow(inputs_df)) {
    
    pop_df <- single_growth(start_x = inputs_df$start_x[i],
                            a = inputs_df$a[i],
                            n = inputs_df$n[i])
    
    current_inputs <- tibble(start_x = rep(inputs_df$start_x[i],
                                           inputs_df$n[i]),
                             a = rep(inputs_df$a[i],
                                     inputs_df$n[i]),
                             n = rep(inputs_df$n[i],
                                     inputs_df$n[i]))
    
    to_add <- bind_cols(current_inputs, pop_df)
    
    outcome_df <- bind_rows(outcome_df, to_add)
  }
  
  # Convert inputs into factors
  outcome_df <- outcome_df %>%
    mutate(start_x = as.factor(start_x),
           a = as.factor(a),
           n = as.factor(n))
  
  outcome_df
}
```

```{r}
blah <- multiple_growth(100, a_opts = c(0.75, 1, 1.25), 10)

blah %>%
  ggplot(aes(x = generation, y = x, col = a)) +
  geom_point() +
  geom_line() +
  scale_x_continuous("Generation", breaks = 1:10) +
  scale_y_continuous("Population") +
  scale_color_viridis_d("r") +
  theme_classic() +
  theme(panel.grid.major = element_line(),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```

This illustrates a general pattern:

* When $r < 1$, population goes to extinction.
* When $r = 1$, population is constant over time.
* When $r > 1$, population grows exponentially.  

A prediction of exponential growth is normally a good sign that your model is a little too unrealistic. If we want a little more realism, then we can add in what population biologists call 'density dependence' and what mathematicians call 'non-linearity'. This gives us an equation of the form:

$$N_{t+1} = N_t(a - bN_{t})$$

In his paper May tells us that we can use a transformation to make the equation simpler to work with. He defines what I'll call the *population proportion* in generation $t$ as

$$x_t = \frac{bN_t}{a}$$

$x_t$ is the population in generation $t$ as a fraction of the maximum possible population. So, if $x_t = 0.5$, then the current population is half as big as the biggest one the environment could support. 

But where does this helpful substitution come from? (probs don't include this)

$$\begin{aligned}
N_{t+1} &= aN_t - bN_{t}^2 \\
\frac{\delta N_{t+1}}{\delta N_{t}} &= a - 2bN_t \\
2bN_t &= a \\
N_t &= \frac{a}{2b} \\
\end{aligned}$$

We can re-write the substitution in terms of $N$, 

$$N_t = \frac{ax_t}{b}$$

and then substitute it into our equation:

$$\begin{aligned}
N_{t+1} &= aN_t - bN_{t}^2 \\
\frac{ax_{t+1}}{b} &= a\frac{ax_t}{b} - b(\frac{ax_t}{b})^2 \\
\frac{ax_{t+1}}{b} &= \frac{a^2x_t}{b} - \frac{a^2x_t^2}{b} \\
ax_{t+1} &= a^2x_t - a^2x_t^2 \\
x_{t+1} &= ax_t - ax_t^2 \\
x_{t+1} &= ax_t(1-x_t)
\end{aligned}$$

What we end up with is **the logistic map**: an equation for the population proportion in generation t+1 as a function of the population proportion in generation t. 

```{r}
log_map_df <- expand.grid(a = c(seq(0.5, 4.5, 1)), x = c(seq(0, 1, 0.01))) %>%
  mutate(y = a*x*(1-x),
         a = as.factor(a))

log_map_df %>%
  ggplot(aes(x = x, y = y, col = a)) +
  geom_line() +
  scale_x_continuous("Pop. prop. at t") +
  scale_y_continuous("Pop.\nprop\nat t+1", breaks = c(seq(0, 1.25, 0.25))) +
  scale_color_viridis_d("a") +
  theme_classic() +
  theme(panel.grid.major = element_line(),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```

Make clear the distinction between linear and non-linear dynamics. It's not just about having a curved line, e.g. exponential growth is linear dynamics.

Play through the maths slowly. 

# Summarising complexity

```{r}
# Single input function
single_map <- function(start_x, a, n) {
  
  # Empty dataframe
  pop_df <- tibble(generation = 1:n,
                   x = numeric(n))
  
  # Add first generation proportion
  pop_df$x[1] <- start_x
  
  # Simulate n generations
  for (i in 2:n) {
    pop_df$x[i] <- a*pop_df$x[i-1]*(1 - pop_df$x[i-1])
  }
  
  # Final dataframe
  pop_df
}

# Multiple input function
multiple_map <- function(x_opts, a_opts, n_opts) {
  
  # Inputs should all be vectors
  
  # Find all possible input permutations
  inputs_df <- expand.grid("start_x" = x_opts,
                           "a" = a_opts,
                           "n" = n_opts)
  
  # Blank outcome df 
  outcome_df <- tibble(start_x = numeric(),
                       a = numeric(),
                       n = numeric(),
                       generation = numeric(), 
                       x = numeric())
  
  # Use single input function for all permutations
  for (i in 1:nrow(inputs_df)) {
    
    pop_df <- single_map(start_x = inputs_df$start_x[i],
                         a = inputs_df$a[i],
                         n = inputs_df$n[i])
    
    current_inputs <- tibble(start_x = rep(inputs_df$start_x[i],
                                           inputs_df$n[i]),
                             a = rep(inputs_df$a[i],
                                     inputs_df$n[i]),
                             n = rep(inputs_df$n[i],
                                     inputs_df$n[i]))
    
    to_add <- bind_cols(current_inputs, pop_df)
    
    outcome_df <- bind_rows(outcome_df, to_add)
  }
  
  # Convert inputs into factors
  outcome_df <- outcome_df %>%
    mutate(start_x = as.factor(start_x),
           a = as.factor(a),
           n = as.factor(n))
  
  outcome_df
}
```

Stable point simulation, a = 2

```{r}
stable_sim <- multiple_map(x_opts = c(0.01, 0.2, 0.4, 0.6, 0.8, 0.99),
                           a_opts = 2,
                           n_opts = 100)

stable_sim %>%
  ggplot(aes(x = generation, y = x, col = start_x)) +
  geom_line() +
  scale_x_continuous("Generation") +
  scale_y_continuous("Pop.\nProp.",
                     limits = c(0, 1),
                     breaks = c(0, .25, .5, .75, 1)) +
  scale_color_viridis_d("Starting\nPoint") +
  theme_classic() +
  theme(panel.grid.major = element_line(),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```

Stable cycle simulation, a = 3

```{r}
cycle_sim <- multiple_map(x_opts = c(0.01, 0.2, 0.4, 0.6, 0.8, 0.99),
                          a_opts = 3,
                          n_opts = 100)

cycle_sim %>%
  ggplot(aes(x = generation, y = x, col = start_x)) +
  geom_line() +
  scale_x_continuous("Generation") +
  scale_y_continuous("Pop.\nProp.",
                     limits = c(0, 1),
                     breaks = c(0, .25, .5, .75, 1)) +
  scale_color_viridis_d("Starting\nPoint") +
  theme_classic() +
  theme(panel.grid.major = element_line(),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```

Chaos simulation, a = 3.65

```{r}
chaos_sim <- multiple_map(x_opts = c(0.49, 0.5, 0.51),
                          a_opts = 3.65,
                          n_opts = 100)

chaos_sim %>%
  ggplot(aes(x = generation, y = x, col = start_x)) +
  geom_line() +
  scale_x_continuous("Generation") +
  scale_y_continuous("Pop.\nProp.",
                     limits = c(0, 1),
                     breaks = c(0, .25, .5, .75, 1)) +
  scale_color_viridis_d("Starting\nPoint") +
  theme_classic() +
  theme(panel.grid.major = element_line(),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```

But, how can we summarise this behaviour in a single graph. Make bifurcation diagram from May's paper.

Can you make bifurcation diagram interactive so the user can zoom in?

# Building non-linear intuitions

Bifurcation diagram is not intuitive at first glance because it's showing long run outcomes as a function of a tuning parameter rather than direct outcomes. Fortunately, there's been more than 40 years' worth of technological advancement since May's paper. This means that even a no-name chump who's not a Baron can build an interactive graph that helps make May's point a little easier to grasp.    

Walk through step by step what they can do with the graph.

Best way to build intuition is to do it yourself. 

So, I made a shiny app that allows you to choose the starting x point for the population.

Make a stripped back version of the real ap just for the blog. 

Better to have the text be in the blog itself. 

<iframe width="1200" height="900" scrolling="no" frameborder="no" align="left" src="https://mitchelfruin.shinyapps.io/log_map/"> </iframe>

Can choose 1 by itself or 2 to compare.

Then click play and watch as we move through different values of the tuning parameter, $a$. Regardless of which starting value(s) you choose, you'll see stability at first with the population converging on a single equilibrium value, then there'll be a transition to 2-period cycling with the population switching between two values, next 4-period cycles, 8-period cycles, and so on, and eventually chaos. 

```{r, eval = F}
# Choose input options
x_rand <- round(runif(3), 2)
r_opts <- c(seq(2, 4, 0.05))
n_opts <- 100

# Use function
plot_df <- multiple_map(x_rand, r_opts, n_opts)
```

```{r, fig.width=9, fig.height=6, eval=F}
# Vary r
plot_df %>%
  plot_ly(x = ~generation,
          y = ~x,
          color = ~start_x,
          hoverinfo = "text+name",
          text = ~paste("Generation:", generation, "<br>",
                        "Population:", round(x, 4)),
          frame = ~a) %>%
  add_lines() %>%
  animation_opts(frame = 800,
                 transition = 100,
                 easing = "linear") %>%
  animation_slider(currentvalue = list(prefix = "a = ",
                                       color = toRGB("black"),
                                       size = 25),
                   hide = FALSE) %>%
  layout(xaxis = list(title = "Generation"),
         yaxis = list(title = "Pop. Prop."))
```

Question: Why do mirrors around 0.5 behave the same?

# Studying non-elephant animals

In the conclusion of his paper, May argues that non-linearity should be taught, and taught early. As he saw it, learning to extend linear mathemetics with Fourier transforms, orthogonal functions, and regression techniques, misleads scientists about the world they're trying to understand. "The mathmetical intuition so developed ill equips the student to confront the bizarre behaviour exhibited by the simplest discrete nonlinear systems.", he wrote. "Yet such nonlinear systems are surely the rule, not the exception, outside the physical sciences."

The crucial notion that nonlinear systems are the norm rather than the exception is summed up in my favourite quote from *Chaos* which comes from the mathematician Stanislaw Ulam. He quipped that "to call the study of chaos 'nonlinear science' was like calling zoology 'the study of nonelephant animals'". That is, it's defining an entire subject in contrast to a niche class. 

The fact that vast swathes of science emphasise linearity has important implications. Something about people thinking in immediate causes from misunderstanding economics book. Poor at incorporating feedback loops or interative thinking. May's final sentence from over 40 years ago still rings true:

> "Not only in research, but also in the everday world of politics and economics, we would all be better off if more people realized that simple nonlinear systems do not necessarily possess simple dynamical properties."

Most things, especially the important things, are greater than the sum of their parts, and the hierarchical reductionism of much of science isn't going to help us understand them even close to well enough.
