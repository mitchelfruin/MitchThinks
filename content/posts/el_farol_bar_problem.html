---
title: "Nobody goes there anymore. It's too crowded."
date: "2019-09-15"
description: "Introduction to the 'El-Farol Bar Problem', a staple of complexity economics, using an agent-based model in R."
tags: ["Agent-based modelling", "Complexity", "Economics"]
slug: el_farol_bar_problem
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    number_sections: true
---


<div id="TOC">
<ul>
<li><a href="#background"><span class="toc-section-number">1</span> Background</a></li>
<li><a href="#model"><span class="toc-section-number">2</span> Model</a><ul>
<li><a href="#set-up"><span class="toc-section-number">2.1</span> Set-up</a></li>
<li><a href="#r-function"><span class="toc-section-number">2.2</span> R function</a></li>
</ul></li>
<li><a href="#simulations"><span class="toc-section-number">3</span> Simulations</a><ul>
<li><a href="#first-glance"><span class="toc-section-number">3.1</span> First glance</a></li>
<li><a href="#comparing-simulations"><span class="toc-section-number">3.2</span> Comparing simulations</a></li>
<li><a href="#changing-the-threshold"><span class="toc-section-number">3.3</span> Changing the threshold</a></li>
<li><a href="#results"><span class="toc-section-number">3.4</span> Results</a></li>
</ul></li>
<li><a href="#why-should-you-care"><span class="toc-section-number">4</span> Why should you care?</a></li>
</ul>
</div>

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>
<p>I’m currently making my way through <em>Complexity and the Economy</em> by W. Brian Arthur. The book’s second chapter is a paper from 1994 called <a href="http://tuvalu.santafe.edu/~wbarthur/Papers/El_Farol.pdf"><em>Inductive Reasoning and Bounded Rationality: The El Farol Problem</em></a>. It’s a fantastic example of how marrying psychological insights and a computational approach can produce novel and intuitive research in economics. So, I thought I’d have a go at using R to reproduce my own (simpler) version of the model described in the paper.</p>
<div id="background" class="section level1">
<h1><span class="header-section-number">1</span> Background</h1>
<p>When Arthur was working at the Santa Fe Institute in 1993, there was a bar called El Farol with Irish music on Thursday nights. He noticed that people would go if they expected few people to be there, but avoided it if they expected it to be too busy.</p>
<p>This is interesting on two fronts.</p>
<ul>
<li><p>First, there is no ‘correct’ deductively rational solution for someone trying to decide whether or not to attend based on their prediction for this week’s attendance. This is because their prediction is based on what they believe other people will do. But, what those other people will do is simultaneously based on their predictions of what everyone except themselves will do. Thus, our focal agent ends up in a kind of reflexive preference loop in which their prediction is a function of every other agent’s prediction which is itself a function of every other agent’s prediction, and so on and so on for as long as you can be bothered to think about it.</p></li>
<li><p>Second, it creates something of a paradox where common beliefs lead to outcomes that cause those beliefs to be proved false. If most people expect many others to attend, then few will show up and the bar will be almost empty. If most people expect nobody to attend, then everyone will show up and it’ll be packed. In both cases the original common belief causes behaviour that means that belief ends up being incorrect.</p></li>
</ul>
<p>These two interesting aspects of the problem should be incorporated in our model if we want it to be useful. So, we need to allow agents to reason inductively by testing a set of hypotheses against reality and choosing to use the one that seems to work best at the present time. We also need to allow our agents to have different sets of hypotheses from one another i.e. different inductive rules about the world that they’re testing.</p>
<p>But if everyone is inductively reasoning from a multitude of different starting beliefs, then won’t we just get completely random attendance over time? Let’s generalise the problem in a toy model similar to Arthur’s to gain some insight into the dynamics of attendance and find out.</p>
</div>
<div id="model" class="section level1">
<h1><span class="header-section-number">2</span> Model</h1>
<div id="set-up" class="section level2">
<h2><span class="header-section-number">2.1</span> Set-up</h2>
<p>100 people decide independently each week whether or not to attend a bar that offers music on a certain night. The music is enjoyable if the bar isn’t too busy, specifically if fewer than <span class="math inline">\(X\%\)</span> of the possible 100 people attend. There’s no way to tell in advance how many people will show up. So, each person goes if they expect fewer than <span class="math inline">\(X\)</span> people to attend but stays home if they think more than <span class="math inline">\(X\)</span> people will be there.</p>
<p>There’s no collusion or prior communication between possible attendees and the choice isn’t affected by whether the person was right or wrong in their beliefs about previous visits. The only info available is a list of the number of people who came in the past weeks.</p>
<p>For example, the past 10 weeks’ attendance might be:</p>
<pre><code>##  [1] 54 41 68 86 70 72 44 70 51 82</code></pre>
<p>Each of the 100 people can individually form different hypotheses they could use to predict the next week’s attendance based on the figures for the past weeks. For example, their hypotheses could include:</p>
<ul>
<li>Predict the same as last week <span class="math inline">\(\rightarrow\)</span> 82 people</li>
<li>Predict a mirror image around 50 of last week <span class="math inline">\(\rightarrow\)</span> 18 people</li>
<li>Predict the average of the past four weeks <span class="math inline">\(\rightarrow\)</span> 62 people</li>
</ul>
<p>Each person decides to go out or stay in according to the currently most accurate hypothesis in their individual set. If their best predictor says that fewer than <span class="math inline">\(X\)</span> people will go, then they go. If their best predictor says that more than <span class="math inline">\(X\)</span> people go, then they don’t.</p>
</div>
<div id="r-function" class="section level2">
<h2><span class="header-section-number">2.2</span> R function</h2>
<p>For brevity I’ve hidden the code where I create the <code>sim_el_farol()</code> function but you can find it <a href="https://github.com/mitchelfruin/MitchThinks/blob/master/content/posts/el_farol_bar_problem.Rmd">here</a>.</p>
<p>What’s important to know is that the function takes two arguments:</p>
<ul>
<li><code>threshold =</code> specifies the crucial cut-off at which people believe the bar is or isn’t too busy. By default this value is 60.</li>
<li><code>n_rounds =</code> specifies the number of weeks to run the simulation for. By default this value is 100.</li>
</ul>
<p>When you run the function it randomly allocates 3 hypotheses like the ones previously described to each of the 100 people. For the first week (when there are no past attendance figures to work with) all of these hypotheses make random predictions for how many people will attend and each person uses a random hypothesis as their ‘best’ prediction. Those who predict that fewer than the <code>threshold</code> value will attend then go out to the bar. Those who predict more that than the <code>threshold</code> value will attend stay home.</p>
<p>Now that we have a figure for the first week’s attendance, the accuracy of each of the 300 hypotheses (remember there are 3 for each person) is calculated as the difference between its predicted attendance and the true figure. Each person then uses their most accurate hypothesis to make a prediction for the next week’s attendance and either goes out or stays home. This cycle repeats for <code>n_rounds</code>.</p>
<p>The function outputs a list of two dataframes. One contains the weekly attendance figures and the other contains each of the 100 individuals’ predictions for every round.</p>
</div>
</div>
<div id="simulations" class="section level1">
<h1><span class="header-section-number">3</span> Simulations</h1>
<div id="first-glance" class="section level2">
<h2><span class="header-section-number">3.1</span> First glance</h2>
<p>OK, let’s see what the simulation spits out with its default settings.</p>
<pre class="r"><code>set.seed(23)
default &lt;- sim_el_farol()</code></pre>
<p><img src="/posts/el_farol_bar_problem_files/figure-html/default%20viz-1.png" width="1056" style="display: block; margin: auto;" /> The blue line in the graph is a linear model of attendance over time. It shows us that, whilst weekly attendance fluctuates, the mean attendance converges on the default threshold value of 60.</p>
</div>
<div id="comparing-simulations" class="section level2">
<h2><span class="header-section-number">3.2</span> Comparing simulations</h2>
<p>What if we compare two default simulations? How consistent is this apparent convergence on the threshold value?</p>
<pre class="r"><code># Sim A
default_A &lt;- sim_el_farol()

# Sim B
default_B &lt;- sim_el_farol()</code></pre>
<p><img src="/posts/el_farol_bar_problem_files/figure-html/unnamed-chunk-1-1.png" width="1056" style="display: block; margin: auto;" /></p>
<p>Re-running with same default parameters changes the fluctuations but average attendance still converges on the threshold value. The different fluctuations are due to the initial random allocation of hypotheses to the individuals. But crucially, we see that no matter how these fluctuations differ the same average behaviour emerges.</p>
</div>
<div id="changing-the-threshold" class="section level2">
<h2><span class="header-section-number">3.3</span> Changing the threshold</h2>
<p>So, from our simulations so far, it seems like the average attendance converges on the threshold value.</p>
<p>Let’s test this by lowering the threshold value to 40.</p>
<pre class="r"><code># Sim A
forty_A &lt;- sim_el_farol(threshold = 40)

# Sim B
forty_B &lt;- sim_el_farol(threshold = 40)</code></pre>
<p><img src="/posts/el_farol_bar_problem_files/figure-html/unnamed-chunk-2-1.png" width="1056" style="display: block; margin: auto;" /></p>
<p>The same behaviour persists but now at the new lower threshold value.</p>
</div>
<div id="results" class="section level2">
<h2><span class="header-section-number">3.4</span> Results</h2>
<p>We’ve seen that the mean attendance seems to converge on the threshold value as Arthur documented in his paper. It’s not immediately obvious why this occurs. It turns out to be because the hypotheses (with the selection pressure provided by agents choosing their most accurate hypothesis) co-evolve into an ecology in which <span class="math inline">\((100-X)\%\)</span> predict values above the threshold and <span class="math inline">\(X\%\)</span> predict values below the threshold.</p>
<p>To illustrate this co-evolutionary process consider the default case with 60 as the threshold value, how do we get a <span class="math inline">\(40:60\)</span> above:below prediction ratio? Well, imagine that instead 50% of hypotheses predicted above 60 for a few weeks in a row. As a result of these consistently high predictions average attendance would fall a little. This lower attendance would make the hypotheses predicting over 60 less accurate and thus less likely to be chosen as an agent’s current best hypothesis. Simultaneously, the lower attendance would make hypotheses predicting low values more likely to be accurate and thus more likely to be chosen for use by an agent. As more agents use hypotheses predicting low values, more agents will choose to attend the bar. Thus causing attendance to rise back up toward the threshold value of 60.</p>
<p>One interesting result which Arthur highlights is that whilst the population of active hypotheses (the ones that are most accurate and thus used in a given round) splits into this <span class="math inline">\((100-X):X\)</span> ratio, it keeps changing its membership continually. He likens this to a forest whose “contours do not change, but whose individual trees do”. To partially illustrate this point I chose 10 individuals randomly from the original default simulation and plotted their individual predictions over the 100 weeks.</p>
<p><img src="/posts/el_farol_bar_problem_files/figure-html/ecology-1.png" width="1056" style="display: block; margin: auto;" /></p>
<p>It’s a bit messy but the plot gets across Arthur’s point that the <span class="math inline">\(40:60\)</span> ratio of hypotheses (that results in an average attendance of 60) is a consequence of different people being above and below the threshold each round. The trees change, but the forest remains the same.</p>
</div>
</div>
<div id="why-should-you-care" class="section level1">
<h1><span class="header-section-number">4</span> Why should you care?</h1>
<p>If you’re a bit of a nerd, one reason you might care about the El Farol problem is that it neatly illustrates the value of numerical approaches to problems where analytical ones are intractable, and the usefulness agent-based modelling, particularly when studying complex systems.</p>
<p>However, a more tangible reason you might care is that this toy model epitomises a surprisingly common type of problem: a coordination problem with a desired limit to coordination. Think about trying to time when you go to the lunchroom each day at work. You want it to be busy enough that there are people to talk to (i.e. you want some coordination), but not so busy that you struggle to find a seat or there’s a huge queue for the microwave (i.e. there’s a desired limit to the coordination). Some of this problem is solved by communication. You can text your mates and arrange to all go to lunch at 14.00 together. In less familiar settings, search engines often try to help us solve problems like these by displaying information about when a destination is usually busiest. But, this extra information doesn’t completely neutralise the El Farol problem. Yes, you do know that based on previous data Tesco is usually pretty empty at 3pm on a Tuesday, but then again so does everyone else. The commonplace nature of this phenomenon is most evident in a quip from the legendary baseball player Yogi Berra about his favourite restaurant Ruggeri’s in St. Louis, “Nobody goes there anymore. It’s too busy.”</p>
</div>
