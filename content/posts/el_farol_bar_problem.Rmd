---
title: "Nobody goes there anymore. It's too crowded."
date: "2019-08-28"
description: "Introduction to the 'El-Farol Bar Problem', a staple of complexity economics."
tags: ["Agent-based modelling", "Complexity", "Economics"]
slug: el_farol_bar_problem
draft: true
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    number_sections: true
---

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r setup, include=FALSE}
# RMarkdown
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", fig.width = 11)

# Packages
library(tidyverse)
library(truncnorm)
```

I'm currently making my way through *Complexity and the Economy* by W. Brian Arthur. The book's second chapter is a paper from 1994 called [*Inductive Reasoning and Bounded Rationality: The El Farol Problem*](http://tuvalu.santafe.edu/~wbarthur/Papers/El_Farol.pdf). It's a fantastic example of how marrying psychological insights and a computational approach can produce novel and intuive research in economics. So, I thought I'd have a go at using R to reproduce my own (simpler) version of the model described in the paper. 

# Background 

When Arthur was working at the Santa Fe Institute in 1993, there was a bar called El Farol with Irish music on thursday nights. He noted that people would go if they expected few people to be there, but avoided it if they expected it to be too busy. This is interesting because it creates something of a paradox where beliefs lead to outcomes that cause those beliefs to be proved false. If most people expect many others to attend, then few will show up and the bar will be empty. If most people expect nobody to attend, then everyone will show up and it'll be packed. In both cases the original belief causes behaviour that means that belief ends up being incorrect.

Something about how there's no deductively rational correct answer here. So even a human acting as closely as possible to the Homo Economicus ideal is forced to use inductive reasoning. But if everyone is inductively reasoning from a multitude of differnt starting beliefs then do we just get completely random attendance over time? If we generalise this phenomenon in a toy model similar to Arthur's we can gain some insight into the dynamics of attendance.  

# Model 
## Set-up

100 people decide independently each week whether or not to attend a bar that offers music on a certain night. The music is enjoyable if the bar isn't too busy, specifically if fewer than $X\%$ of the possible 100 people attend. There's no way to tell in advance how many people will show up. So, each person goes if they expect fewer than $X$ people to attend but stays home if they think more than $X$ people will be there.

There's no collusion or prior communication between possible attendees and the choice isn't affected by whether the person was right or wrong in their beliefs about previous visits. The only info available is a list of the number of people who came in the past weeks.

For example, the past 10 weeks' attendance might be:

```{r attendance eg}
# Example attendance
set.seed(23)
round(truncnorm::rtruncnorm(10, 0, 100, 50, 20), 0)
```

Each of the 100 people can individually form different hypotheses they could use to predict the next week's attendance based on the figures for the past weeks. For example, their hypotheses could include:

* Predict the same as last week $\rightarrow$ 82 people
* Predict a mirror image around 50 of last week $\rightarrow$ 18 people
* Predict the average of the past four weeks $\rightarrow$ 62 people

Each person decides to go out or stay in according to the currently most accurate hypothesis in their individual set. If their best predictor says that fewer than $X$ people will go, then they go. If their best predictor says that more than $X$ people go, then they don't. 

## R function 

For brevity I've hidden the code where I create the `sim_el_farol()` function but you can find it [here](https://github.com/mitchelfruin/MitchThinks/blob/master/content/posts/el_farol_bar_problem.Rmd). 

```{r define sim function}
sim_el_farol <- function(threshold = 60, n_rounds = 100) {
  
  ## Define mean-based hypotheses -----
  
  # Mean of previous 1 week
  mean_1 <- function(x) {
    if(length(x) < 1) {sample(c(1:100), 1)} 
    else{mean(x[length(x)])}
  }
  
  # Mean of previous 2 weeks
  mean_2 <- function(x) {
    if(length(x) < 2) {sample(c(1:100), 1)} 
    else{mean(c(x[length(x)], x[length(x) - 1]))}
  }
  
  # Mean of previous 3 weeks
  mean_3 <- function(x) {
    if(length(x) < 3) {sample(c(1:100), 1)} 
    else{mean(c(x[length(x)], x[length(x) - 1], x[length(x) - 2]))}
  }
  
  # Mean of previous 4 weeks
  mean_4 <- function(x) {
    if(length(x) < 4) {sample(c(1:100), 1)} 
    else{mean(c(x[length(x)],
                x[length(x) - 1],
                x[length(x) - 2],
                x[length(x) - 3]))}
  }

  # Mean of previous 5 weeks
  mean_5 <- function(x) {
    if(length(x) < 4) {sample(c(1:100), 1)} 
    else{mean(c(x[length(x)],
                x[length(x) - 1],
                x[length(x) - 2],
                x[length(x) - 3],
                x[length(x) - 4]))}
  }
  
  ## Define cycle-based hypotheses -----
  
  # 1 week cycle 
  cycle_1 <- function(x){
    if(length(x) < 1) {sample(c(1:100), 1)} 
    else{x[length(x)]}
  }
  
  # 2 week cycle 
  cycle_2 <- function(x){
    if(length(x) < 2) {sample(c(1:100), 1)} 
    else{x[length(x) - 1]}
  }
  
  # 3 week cycle 
  cycle_3 <- function(x){
    if(length(x) < 3) {sample(c(1:100), 1)} 
    else{x[length(x) - 2]}
  }
  
  # 4 week cycle 
  cycle_4 <- function(x){
    if(length(x) < 4) {sample(c(1:100), 1)} 
    else{x[length(x) - 3]}
  }
  
  # 5 week cycle 
  cycle_5 <- function(x){
    if(length(x) < 5) {sample(c(1:100), 1)} 
    else{x[length(x) - 4]}
  }
  
  ## Define mirror-based hypotheses -----
  
  # 1 week mirror 
  mirror_1 <- function(x) {
    if(length(x) < 1) {sample(c(1:100), 1)} 
    else{
      mirror <- x[length(x)] - 50
      ifelse(mirror > 0, return(50 - mirror), return(50 + abs(mirror)))
    }
  }
  
  # 2 week mirror 
  mirror_2 <- function(x) {
    if(length(x) < 2) {sample(c(1:100), 1)} 
    else{
      mirror <- x[length(x) - 1] - 50
      ifelse(mirror > 0, return(50 - mirror), return(50 + abs(mirror)))
    }
  }
  
  # 3 week mirror 
  mirror_3 <- function(x) {
    if(length(x) < 3) {sample(c(1:100), 1)} 
    else{
      mirror <- x[length(x) - 2] - 50
      ifelse(mirror > 0, return(50 - mirror), return(50 + abs(mirror)))
    }
  }
  
  # 4 week mirror 
  mirror_4 <- function(x) {
    if(length(x) < 4) {sample(c(1:100), 1)} 
    else{
      mirror <- x[length(x) - 3] - 50
      ifelse(mirror > 0, return(50 - mirror), return(50 + abs(mirror)))
    }
  }
  
  # 5 week mirror 
  mirror_5 <- function(x) {
    if(length(x) < 5) {sample(c(1:100), 1)} 
    else{
      mirror <- x[length(x) - 4] - 50
      ifelse(mirror > 0, return(50 - mirror), return(50 + abs(mirror)))
    }
  }
  
  ## Group hypotheses -----
  
  hypotheses <- list("1wk cycle" = cycle_1,
                     "2wk cycle" = cycle_2,
                     "3wk cycle" = cycle_3,
                     "4wk cycle" = cycle_4,
                     "5wk cycle" = cycle_5,
                     "1wk mean" = mean_1,
                     "2wk mean" = mean_2,
                     "3wk mean" = mean_3,
                     "4wk mean" = mean_4,
                     "5wk mean" = mean_5,
                     "1wk mirror" = mirror_1,
                     "2wk mirror" = mirror_2,
                     "3wk mirror" = mirror_3,
                     "4wk mirror" = mirror_4,
                     "5wk mirror" = mirror_5)
  
  
  ## Sim -----
  
  # Assign 3 hypotheses to each individual
  predictors_df <- 
    tibble(id = rep(c(1:100), 3),
           predictor = sample(hypotheses, 300, replace = T)) %>%
    arrange(id) %>%
    mutate(current_accuracy = 0)
  
  # Define function for finding current most accurate predictor
  find_predictor <- function(predictors_df) {
    best_predictors <- predictors_df %>%
      group_by(id) %>%
      summarise(best_accuracy = min(current_accuracy),
                predictor = predictor[which.min(current_accuracy)])
  }
  
  # Empty df for predictions
  predictions_df <- tibble(id = c(1:100))
  
  # Empty vector for weekly attendance
  attendance <- NULL

  # Run it
  for (i in 1:n_rounds) {
    
    # Make predictions using all hypotheses
    all_predictions <- lapply(predictors_df$predictor,
                              function(f) f(attendance))
    all_predictions <- unlist(all_predictions)
    
    # Find best_predictors
    best_predictors <- find_predictor(predictors_df)
    
    # Make predictions using best hypotheses
    best_predictions <- lapply(best_predictors$predictor,
                               function(f) f(attendance))
    best_predictions <- unlist(best_predictions)
    
    # Add best predictions to the predictions_df in the right week
    predictions_df[, 1+i] <- best_predictions
    
    # Count how many individuals will go to the bar that week 
    present_attendance <- sum(best_predictions <= threshold)
    
    # Add present attendance to the attendance vector
    attendance <- c(attendance, present_attendance)
    
    # Update the current accuracy for the predictors
    predictors_df <- predictors_df %>%
      mutate(prediction = all_predictions,
             current_accuracy = current_accuracy + 
               abs(prediction - present_attendance)) %>%
      select(-prediction)
    
  }
  ## Output -----
  
  # Overall attendance
  attendance_df <- tibble(week_no = 1:n_rounds, attendance = attendance)
  
  # Add attendance and individual predictions to environment
  list("predictions" = predictions_df, "attendance" = attendance_df)
  
}
```

What's important to know is that the function takes two arguments:

* `threshold = ` specifies the crucial cut-off at which people believe the bar is or isn't too busy. By default this value is 60.
* `n_rounds = ` specifies the number of weeks to run the simulation for. By default this value is 100. 

When you run the function it randomly allocates 3 hypotheses like the ones previously described to each of the 100 people. For the first week (when there are no past attendance figures to work with) all of these hypotheses make random predictions for how many people will attend and each person uses a random hypothesis as their 'best' prediction. Those who predict fewer than the `threshold` value will attend go to the bar. Those who predict more than the `threshold` value will attend stay home. 

Now we have a figure for the first week's attendance the accuracy of each of the 300 hypotheses (remember there are 3 for each person) is calculated as the difference between its predicted attendance and the true figure. Each person then uses their most accurate hypothesis so far to make a prediction for the next week's attendance and the cycle repeats for `n_rounds`. 

The function outputs a list of two dataframes. One contains the weekly attendance figures and the other contains each of the 100 individuals' predictions for every round. 

# Simulations
## First glance 

OK, let's see what the simulation spits out with its default settings. 

```{r first glance, echo=TRUE}
set.seed(23)
default <- sim_el_farol()
```

```{r default viz}
ggplot(default$attendance, aes(x = week_no, y = attendance)) +
  geom_line() +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  scale_x_continuous("Week No.",
                     breaks = c(seq(0, 100, 20))) +
  scale_y_continuous("Attendance", 
                     breaks = c(seq(0, 100, 10)),
                     limits = c(0, 100)) +
  theme_classic() +
  theme(panel.grid.major = element_line())
```
The blue line in the graph is a linear model of attendance over time. It shows us that whilst weekly attendance fluctuates the mean attendance converges on the default threshold value of 60.

## Comparing simulations

What if we compare two default simulations? How consistent is this apparent convergence on the threshold value?

```{r two default sims, echo = TRUE}
# Sim A
default_A <- sim_el_farol()

# Sim B
default_B <- sim_el_farol()
```

```{r}
# Viz A
plot_A <- ggplot(default_A$attendance,
                 aes(x = week_no, y = attendance)) +
  geom_line() +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  scale_x_continuous("Week No.",
                     breaks = c(seq(0, 100, 20))) +
  scale_y_continuous("Attendance", 
                     breaks = c(seq(0, 100, 10)),
                     limits = c(0, 100)) +
  theme_classic() +
  theme(panel.grid.major = element_line())

# Viz B
plot_B <- ggplot(default_B$attendance,
                 aes(x = week_no, y = attendance)) +
  geom_line() +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  scale_x_continuous("Week No.",
                     breaks = c(seq(0, 100, 20))) +
  scale_y_continuous("Attendance", 
                     breaks = c(seq(0, 100, 10)),
                     limits = c(0, 100)) +
  theme_classic() +
  theme(panel.grid.major = element_line())

# Plot side by side
cowplot::plot_grid(plot_A, plot_B, labels = "AUTO")
```

Re-running with same default parameters changes the fluctuation but average attendance still converges on the threshold value. The different fluctuations are due to the initial random allocation of hypotheses amongst individuals. But we see that no matter how these fluctuations differ the same average behaviour emerges. 

## Changing the threshold

So, from our simulations so far, it seems like the average attendance converges on the threshold value. Let's test this by altering the threshold value to 40.  

```{r two 40 sims, echo = TRUE}
# Sim A
forty_A <- sim_el_farol(threshold = 40)

# Sim B
forty_B <- sim_el_farol(threshold = 40)
```

```{r}
# Viz A
plot_A <- ggplot(forty_A$attendance,
                 aes(x = week_no, y = attendance)) +
  geom_line() +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  scale_x_continuous("Week No.",
                     breaks = c(seq(0, 100, 20))) +
  scale_y_continuous("Attendance", 
                     breaks = c(seq(0, 100, 10)),
                     limits = c(0, 100)) +
  theme_classic() +
  theme(panel.grid.major = element_line())

# Viz B
plot_B <- ggplot(forty_B$attendance,
                 aes(x = week_no, y = attendance)) +
  geom_line() +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  scale_x_continuous("Week No.",
                     breaks = c(seq(0, 100, 20))) +
  scale_y_continuous("Attendance", 
                     breaks = c(seq(0, 100, 10)),
                     limits = c(0, 100)) +
  theme_classic() +
  theme(panel.grid.major = element_line())

# Plot side by side
cowplot::plot_grid(plot_A, plot_B, labels = "AUTO")
```

The same behaviour persists but now at the new threshold value.

## Changing the timespan

Zooming in looks almost fractal 

## Results

Mention ecology?

It's different people each time. 

```{r ecology}
set.seed(45)
id_index <- sample(1:100, 5)
individuals <- default$predictions[id_index, ]
names(individuals) <- c("id", 1:100)
individuals_plot <- reshape2::melt(individuals,
                                   id.vars = "id",
                                   variable.name = "week") %>%
  mutate(id = as.factor(id),
         week = as.numeric(week))

ggplot(individuals_plot, aes(x = week, y = value, col = id)) +
  geom_line() +
  geom_point(size = 0.75) +
  geom_hline(yintercept = 60, lty = 2) +
  scale_color_discrete("ID") +
  scale_x_continuous("Week No.",
                     breaks = c(seq(0, 100, 20))) +
  scale_y_continuous("Predicted Attendance", 
                     breaks = c(seq(0, 100, 10)),
                     limits = c(0, 100)) +
  theme_classic() +
  theme(panel.grid.major = element_line())
```

# Why should you care?

It exemplifies a common type of problem. Seems like the model is unrealistic but link to 'busy at these times' info on google, in exactly the situation the model describes. You know based on previous data, Tesco is usually empty at 3pm on a tuesday, but so does everyone else. 

It shows the value of numerical solutions to problems when analytical ones become too hard or are impossible (in this case because of inductive reasoning). 

Conclusion should relate to the Yogi Berra quote about his favourite restaurant in St. Louis, Ruggeri's. 
