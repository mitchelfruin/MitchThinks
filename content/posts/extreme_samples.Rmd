---
title: "Slow thinking about small samples."
author: "Mitchel Fruin"
date: "2019-10-26"
description: "Add description."
tags: ["Behavioural Economics", "Probability and Statistics"]
slug: small_samples
draft: true
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    number_sections: true
---

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r setup, include=FALSE}
# RMarkdown
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8.09,
                      fig.height = 5)

# Packages
library(tidyverse)
library(magrittr)
library(plotly)
library(funnelR)
```

Wainer and Zwerling cancer example. 

Do me the favour of thinking about how you would answer these two questions. 

# Small samples are more extreme

Kahneman comments on this paper in *Thinking, Fast and Slow*. One draws 4, One draws 7

Adapt his example (is it from one of their actual papers?).

Assume sex ratio is 50:50. One hospital maternity ward has 4 births each day, another has 8. We're interested in extreme results. In this context, that equates to a 'single-sex day' where all the babies born are the same sex. Intuitively we understand that single sex days will happen more often on the smaller ward for the same reason that we know instinctively that it's easier to flip four heads or tails in a row than it is 8. But what we don't seem to understand so intuitively is how much more often extreme events will happen in the small samples. Actually think about what your guess is for the following question, how much more often will the smaller ward have a single-sex day  than the larger ward?

We can find an exact answer to this question by comparing two different binomial distributions.

$$\begin{aligned}
X&\sim \text{B}(4, 0.5) \\
Y&\sim \text{B}(7, 0.5)
\end{aligned}$$

```{r}
big_exact <- dbinom(7, 7, 0.5)*2
small_exact <- dbinom(4, 4, 0.5)*2
small_exact/big_exact
```

If that's too much maths, then we can run a simulation to get an approximate answer instead.

Say we simulate 1000 days at each ward. 

```{r}
# Simulate 1000 days at each hospital
set.seed(23)
small_hosp <- rbinom(1000, 4, 0.5) #  X is no. of boys
big_hosp <- rbinom(1000, 8, 0.5)
sims <- tibble(hospital = c(rep("Small", 1000), rep("Big", 1000)),
               births = c(small_hosp, big_hosp))
```

Plot the frequencies of the number of boys born each day

```{r}
sims %>%
  ggplot(aes(x = births)) +
  geom_histogram() +
  facet_wrap(~ hospital, scales = "free_x") +
  theme_bw() +
  theme(panel.grid.major = element_line(),
        axis.line.y = element_line())
```

```{r}
# Find the single sex days
small_extreme <- sum(small_hosp == 4| small_hosp == 0)
big_extreme <- sum(big_hosp == 8| big_hosp == 0)
```

130 days at the small ward, 8 days at the big one. This gives us a ratio of 16.25, which is pretty damn close to the exact answer. 

So, in this instance our fast thinking lead us astray a little and we tend to underestimate the ratio. What can we do about it?

# Funnel plots to the rescue

I recently came across an example from the UK in a similar vein to the one discussed by Wainer and Zwerling.

In *The Art of Statistics* David Spiegelhalter discusses a piece of analysis by Paul Barden. Barden saw a BBC news report headlined "3-fold variation in bowel cancer rate" and decided to dig a little deeper. 

The date are discussed in a blog by Paul Barden on the [Understanding Uncertainty site](https://understandinguncertainty.org/three-fold-variation-uk-bowel-cancer-death-rates) and on his [own blog](https://pb204.blogspot.com/2011/09/im-grateful-to-david-spiegelhalter-of.html). Note that data for Wales is not included, as it was reported for the whole of Wales rather than by region.

```{r}
bowel_data <- read.csv("https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/09-2-funnel-bowel-cancer/09-2-bowel-cancer-data-x.csv")

mean_prop <- sum(bowel_data$n)/sum(bowel_data$d)
max_props <- max(bowel_data$n/bowel_data$d)

# Numerator must be called n, denomnator d
funnel_limits <- fundata(input = bowel_data,
                         benchmark = mean_prop,
                         alpha = 0.95,
                         alpha2 = 0.998,
                         method = 'approximate',
                         step = 100)

glasgow <- subset(bowel_data, District == "Glasgow City")  # identify Glasgow City in data frame
```

He found that there is 3-fold variation, but there isn't anything more interesting going on than the small hospital vs. big hospital example we just played through.

Plot one graph showing crazy variation (can you find original graph from BBC?):

```{r}
# plot
```

But, as we established above, small samples generate extreme results more often. So, we need to incorporate population size in our analysis somehow. Haven’t we already done that by making it a rate?

The easiest way to do this is with a ‘funnel plot’ which visualises the mean and expected variance from this mean given sample size. As sample size gets bigger, CI gets smaller because it’s calculation uses $\sqrt{n}$ in the denominator.

```{r}
bowel_data %<>% mutate(prop_death = n/d)

# Lowest pop 31332, find funnel limits at 31001
up_limit <- filter(funnel_limits, d == 31001) %>%
  pull(up2)
low_limit <- filter(funnel_limits, d == 31001) %>%
  pull(lo2)

ggplot() +
  geom_point(aes(x = d, y = prop_death), data = bowel_data) +
  geom_hline(yintercept = mean_prop) +
  geom_ribbon(aes(x = d, ymin = lo, ymax = up),
              data = funnel_limits,
              fill = "blue",
              alpha = 0.4) +
  geom_ribbon(aes(x = d, ymin = lo2, ymax = up2),
              data = funnel_limits,
              fill = "purple",
              alpha = 0.2) +
  scale_y_continuous("Annual bowel cancer mortality rate per 100,000",
                     breaks = 5*(0:8)/100000,
                     labels = 5*(0:8),
                     limits = c(low_limit, up_limit)) + 
  scale_x_continuous("Population (100,000's)",
                     breaks = 100000*(0:14),
                     labels = 0:14,
                     limits = c(0, max(bowel_data$d))) +
  theme_classic() +
  theme(panel.grid.major = element_line()) + 
  annotate("text", 
           x = glasgow$d,
           y = glasgow$n/glasgow$d,
           label = "Glasgow City",
           hjust = -0.05,
           vjust = 1)
```

How many of the observations fall within the bounds? Suspiciously good match 

# Stats as slow thinking

The metaphor of two modes of thinking, one fast and intuitive but prone to some predictable mistakes and the other slow and deliberate but sometimes hard to engage, has become fairly mainstream since Kahneman’s best-seller and the popularisation of behavioural economics. This metaphor is often used when discussing the ‘mistakes’ people are susceptible to. However, accepting the fallibility of our own reasoning is just the start of good decision-making, not an end in itself. The next step is to consider just what kind of slow thinking we should employ in a given situation.

Simplest pros vs. cons.

Then maybe something like an improper linear model.

But can also be proper statistical analysis.

This is where my interest in behavioural economics meets my interest in data science. Data science is often about painfully slow thought, taking the right care.

Relationship between behavioural economics and data science.

Particularly when making large scale policy decisions, ensuring the right kind of slow thinking is crucial. Bill & Melinda Gates Foundation example of it gone wrong.
