---
title: "Slow thinking about small samples."
author: "Mitchel Fruin"
date: "2019-10-26"
description: "Add description."
tags: ["Behavioural Economics", "Probability and Statistics"]
slug: small_samples
draft: true
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    number_sections: true
---

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r setup, include=FALSE}
# RMarkdown
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8.09,
                      fig.height = 5)

# Packages
library(tidyverse)
library(magrittr)
library(plotly)
library(funnelR)
```

Wainer and Zwerling cancer example. 

# Small samples are more extreme

Kahneman comments on this paper in *Thinking, Fast and Slow*. One draws 4, One draws 7

Adapt his example (is it from one of their actual papers?).

Assume sex ratio is 50:50. One hospital maternity ward has 4 births each day, another has 8. We're interested in extreme results. In this context, that equates to a 'single-sex day' where all the babies born are the same sex. Intuitively we understand that single sex days will happen more often on the smaller ward for the same reason that we know instinctively that it's easier to flip four heads or tails in a row than it is 8. But what we don't seem to understand so intuitively is how much more often extreme events will happen in the small samples. Actually think about what your guess is for the following question, how much more often will the smaller ward have a single-sex day  than the larger ward?

We can find an exact answer to this question by comparing two different binomial distributions.

Add equations here.

```{r}
big_exact <- dbinom(8, 8, 0.5)*2
small_exact <- dbinom(4, 4, 0.5)*2
small_exact/big_exact
```

If that's too much maths, then we can run a simulation to get an approximate answer instead.

Say we simulate 1000 days at each ward. 

```{r}
# Simulate 1000 days at each hospital
set.seed(23)
small_hosp <- rbinom(1000, 4, 0.5) #  X is no. of boys
big_hosp <- rbinom(1000, 8, 0.5)
sims <- tibble(hospital = c(rep("Small", 1000), rep("Big", 1000)),
               births = c(small_hosp, big_hosp))
```

Plot the frequencies of the number of boys born each day

```{r}
sims %>%
  ggplot(aes(x = births)) +
  geom_histogram() +
  facet_wrap(~ hospital, scales = "free_x") +
  theme_bw() +
  theme(panel.grid.major = element_line(),
        axis.line.y = element_line())
```

```{r}
# Find the single sex days
small_extreme <- sum(small_hosp == 4| small_hosp == 0)
big_extreme <- sum(big_hosp == 8| big_hosp == 0)
```

130 days at the small ward, 8 days at the big one. This gives us a ratio of 16.25, which is pretty damn close to the exact answer. 

So, in this instance our fast thinking lead us astray a little and we tend to underestimate the ratio. What can we do about it?

# Funnel plots to the rescue

I recently came across an example from the UK in a similar vein to the one discussed by Wainer and Zwerling.

In *The Art of Statistics* David Spiegelhalter discusses a piece of analysis by Paul Barden. Barden saw a BBC news report headlined "3-fold variation in bowel cancer rate" and decided to dig a little deeper. 

The date are discussed in a blog by Paul Barden on the [Understanding Uncertainty site](https://understandinguncertainty.org/three-fold-variation-uk-bowel-cancer-death-rates) and on his [own blog](https://pb204.blogspot.com/2011/09/im-grateful-to-david-spiegelhalter-of.html). Note that data for Wales is not included, as it was reported for the whole of Wales rather than by region.

```{r}
bowel_data <- read.csv("https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/09-2-funnel-bowel-cancer/09-2-bowel-cancer-data-x.csv")

mean_prop <- sum(bowel_data$n)/sum(bowel_data$d)
max_props <- max(bowel_data$n/bowel_data$d)

# Numerator must be called n, denomnator d
funnel_limits <- fundata(input = bowel_data,
                         benchmark = mean_prop,
                         alpha = 0.95,
                         alpha2 = 0.998,
                         method = 'approximate',
                         step = 100)

glasgow <- subset(bowel_data, District == "Glasgow City")  # identify Glasgow City in data frame
```

He found that there is 3-fold variation, but there isn't anything more interesting going on than the small hospital vs. big hospital example we just played through.

Plot one graph showing crazy variation (can you find original graph from BBC?):

```{r}
# plot
```

But, as we saw above, small samples are more extreme. So, we need to include population size in our analysis. The easiest way to do this is with a 'funnel plot'. 

```{r}
bowel_data %<>% mutate(prop_death = n/d)

# Lowest pop 31332, find funnel limits at 31001
up_limit <- filter(funnel_limits, d == 31001) %>%
  pull(up2)
low_limit <- filter(funnel_limits, d == 31001) %>%
  pull(lo2)

ggplot() +
  geom_point(aes(x = d, y = prop_death), data = bowel_data) +
  geom_hline(yintercept = mean_prop) +
  geom_ribbon(aes(x = d, ymin = lo, ymax = up),
              data = funnel_limits,
              fill = "blue",
              alpha = 0.4) +
  geom_ribbon(aes(x = d, ymin = lo2, ymax = up2),
              data = funnel_limits,
              fill = "purple",
              alpha = 0.2) +
  scale_y_continuous("Annual bowel cancer mortality rate per 100,000",
                     breaks = 5*(0:8)/100000,
                     labels = 5*(0:8),
                     limits = c(low_limit, up_limit)) + 
  scale_x_continuous("Population (100,000's)",
                     breaks = 100000*(0:14),
                     labels = 0:14,
                     limits = c(0, max(bowel_data$d))) +
  theme_classic() +
  theme(panel.grid.major = element_line()) + 
  annotate("text", 
           x = glasgow$d,
           y = glasgow$n/glasgow$d,
           label = "Glasgow City",
           hjust = -0.05,
           vjust = 1)
```

How many of the observations fall within the bounds? Suspiciously good match 

# Stats as slow thinking

Relationship between behavioural economics and data science. 

In fact, careful data analysis might be the slowest thinking of all. 

Bill & Melinda Gates Foundation example of it gone wrong. 
