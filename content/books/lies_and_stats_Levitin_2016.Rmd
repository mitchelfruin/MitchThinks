---
title: "A Field Guide to Lies & Stats - Levitin 2016"
date: "2019-08-10"
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

If you want to see the full code for this post then you can find the .Rmd file [here](https://github.com/mitchelfruin/MitchThinks/blob/master/content/books/lies_and_stats_Levitin_2016.Rmd).

# Summary

Levitin provides a solid compilation of mistakes to avoid and tricks to look out for when producing and reading research respectively. However, given his background in  psychology and neuroscience, I was hoping for a more general thread pulling the examples together concerning *why* the human brain is 'fooled' by particular ways of presenting information rather than just *what* those presentations are.

The book's main strength was its illustrative examples which stuck in my mind and comprise the rest of this post.

# Is the stat plausible?

The first thing to check when presented with a statistic, especially out of context, is whether the claim is even plausible in the first place. 

## Back of the napkin calculations

One way to check plausibility is a quick on-the-fly calculation.

Say, for example, we run into someone claiming that "in the 35 years since marijuana laws stopped being enforced in California, the number of marijuana smokers has doubled every year". 

Well, even if we start with the most generous estimate that initially there was only 1 lonely pothead in California, then if that number doubled every year for 35 years:

$$\begin{aligned}
\text{Year 1:} \;\; 1 &= 2^0 \\
\text{Year 2:} \;\; 2 &= 2^1 \\
\text{Year 3:} \;\; 4 &= 2^2 \\
\vdots \\
\text{Year 35:} \;\; &= 2^{34} \\
\end{aligned}$$

So let's see just how many marijuana smokers we've currently got if that stat holds:
```{r potheads}
# Too high to count that high
2^34
```

Just over 17.1 billion people, which seems more than a little high. 

## Remember the real world

In a maths lecture at uni the professor began by writing a complicated looking formula on the board and told us that it described the time taken for a bath to empty as a function of its volume and the diameter of the plughole. He then asked us to raise our hand if we believed the bath would ever empty. When only a few of us sheepishly did so he shouted up into the lecture theatre, "have you fucking taken a bath before?". His point was, just because we're dealing with supposedly scary maths doesn't mean you should ignore common sense. You've taken a bath before, you know the answer, whether you understand the formula or not. 

These kind of 'wait, but we're still talking about the real world' checks can sometimes help assess the plausibility of a statistic. For example, in 1993 New Jersey adopted a 'family cap' law that denied additional benefits to mothers who have children while already on welfare. After two months, legislators were celebrating its success as births had already fallen by 16%. The only problem being that human pregnancies generally take a bit longer than 2 months. 

# Does the average mean anything?

## Mean, median and mode

## All averages aren't made equal

Relative percentage change vs. absolute percentage change (i.e. percentage points) vs. natural frequencies. 

## Bimodal distributions

Humans on average have one testicle. Meaningless to compute an average on a metric over a whole population where there are important differences. 

# Are the axes leading you astray?

## Truncated

Exaggerate between-group differences. If 0 is a plausible value it should be included. 

## Discontinuous

Easy to make line graph jump. 

## Elongated

Include axis space beyond the range of your data to make changes look more extreme. 

## Duplicate

If they measure the same thing then you can fudge the graph however you want. Only use if they're different units and you want to make within axis comparisons not between. E.g. planned parenthood Jason chaffetz. 

# Dodgy reporting 

## Hiding a decrease in the cumulative

Tim Cook iPhone sales. 

## Unrelated plotting

Spurious time series. 

## Irrelevant differences

Just increase N enough to lower the p-value until it's 'significant'.

## Comparing apples and oranges 

**Amalgamating**: group samples that differ on an important dimension to make the stat more shocking. "50% of 10-18 year olds are sexually active". 

**Subdividing**: split things inconsistently to make your point more impressive. Divide all causes of death into subcategories apart from the one you want to promote. Should be equal bins (either by percentile of absolute).  

# High probability that's not quite right

## Conditional probabilities

Sometimes probability is unintuitive. Birthday puzzle. 

**Types.** Classic, based on symmetry. Frequentist, based on experience. Subjective, based on best guess. 

**Independent probabilities.** Should multiply. 

**Dependent/Conditional probabilities.** Sally Clark had two children dies from SIDS. Dr. Meadow, pediatrician, said there's 1 in 8543 chance of having a child die from SIDS giving a 1 in 73mil chance of having two children die from it. Potential genetic, environmental dependence ignored and she was sentenced for 3 years. Visualise Bayesian updating with a contingency table. Base rate neglect in medical test results. 

## The prosecutor's fallacy

**Prosecutor's fallacy:** False belief that conditional probabilities are invertible when in fact they aren't.

$$Pr(A|B) \neq Pr(B|A)$$

Forensics may compute that probability of blood at scene matching the defendent's by chance is 1%. That doesn't mean there's a 1% chance the defendent is innocent. We have been told the probability of a match given innocence, we want the probability of innocence given a match, but they don't equate:

$$Pr(\text{match}|\text{innocent}) \neq Pr(\text{innocent}|\text{match})$$

## Bayesian updating

